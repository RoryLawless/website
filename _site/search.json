[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rory Lawless",
    "section": "",
    "text": "The basics of DuckDB in R\n2025-03-30\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Rory Lawless",
    "section": "",
    "text": "The basics of DuckDB in R\n2025-03-30\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Rory Lawless",
    "section": "About Me",
    "text": "About Me\nI am a data analyst with experience across the non-profit and public sectors. The most important part of my work, especially within my public sector roles, is using data to build deep understanding of the domains I work in, which both informs my own analysis and helps the audience develop meaningful narratives."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Rory Lawless",
    "section": "",
    "text": "I am a data analyst with experience across the non-profit and public sectors. The most important part of my work, especially within my public sector roles, is using data to build deep understanding of the domains I work in, which both informs my own analysis and helps the audience develop meaningful narratives."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Rory Lawless",
    "section": "",
    "text": "I am a data analyst with experience across the non-profit and public sectors. The most important part of my work, especially within my public sector roles, is using data to build deep understanding of the domains I work in, which both informs my own analysis and helps the audience develop meaningful narratives."
  },
  {
    "objectID": "about.html#résumé",
    "href": "about.html#résumé",
    "title": "Rory Lawless",
    "section": "Résumé",
    "text": "Résumé\n\nWork\nData Analyst, Office of the Deputy Mayor for Education (DME)\nWashington, DC. 2022–Present\n\nContributed analysis for key publications, including 2023 Master Facilities Plan.\nImproved school enrollment projections processes, leading cross-agency collaboration.\nInformation officer and racial justice & equity team member.\n\nAssociate Data Analyst, Financial Conduct Authority (FCA)\nLondon, UK. 2020–2022\n\nProduced insightful and impactful analysis for internal and external audiences in support of high profile, politically sensitive publications.\nData collection and extraction using SQL and web scraping techniques.\nDeveloped best practice for data management and replicable analysis through training and providing ad hoc advice and support.\n\nResearch Assistant - Data, Child Outcomes Research Consortium (CORC)\nLondon, UK. 2018–2020\n\nSupporting services to submit data to the organization, including one-to-one support as well as organizing and hosting webinars.\nRedesigned data collection tools to improve quality of submissions and automated internal data validation processes.\n\n\n\nEducation\nMSc Democracy and Comparative Politics, Distinction\nUniversity College London. London, UK. 2016\nBA (Hons.) Politics, Upper Second Class\nRoyal Holloway, University of London. Surrey, UK. 2013"
  },
  {
    "objectID": "posts/the-basics-of-duckdb-in-r/index.html",
    "href": "posts/the-basics-of-duckdb-in-r/index.html",
    "title": "The basics of DuckDB in R",
    "section": "",
    "text": "Over the past year, DuckDB has gradually become an important part of my data science workflow - at first clumsily, then seamlessly. I don’t typically work with large datasets, however, integrating DuckDB has addressed some of my frustrations, especially when dealing with hardware limitations and moderately-sized but inefficiently stored data. With this in mind, here are two major benefits I’ve found since integrating DuckDB into my workflow."
  },
  {
    "objectID": "posts/the-basics-of-duckdb-in-r/index.html#handling-larger-than-memory-data",
    "href": "posts/the-basics-of-duckdb-in-r/index.html#handling-larger-than-memory-data",
    "title": "The basics of DuckDB in R",
    "section": "Handling larger-than-memory data",
    "text": "Handling larger-than-memory data\nAs noted, I don’t work with very large data often but I still run into annoying issues caused by repeated reloading of data after making mistakes. Now, this is not an issue for a .csv file containing a few hundred rows and, for larger files or those stored in legacy formats, I could add a “backup” step to my code, like so:\n\ndata &lt;- read.csv(\"some-data-file.csv\")\ndata_backup &lt;- data\n\n# Do some work on data, maybe make a mistake...\n\ndata &lt;- data_backup\n\nThis works fine, but I consider it an anti-pattern and ought to, in my opinion, be avoided. Instead of adding this extra step - likely increasing the memory used in the R session - you can use DuckDB to directly query files stored on disk, without having to load them into memory first.\n\nlibrary(tidyverse)\nlibrary(duckdb)\n\n# Create a DuckDB connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Write a SQL query to read data directly from the CSV file\ndata &lt;- dbGetQuery(\n    con,\n    \"SELECT col_1, col_2, col_4, col_10\n    FROM 'some-data-file.csv'\n    WHERE col_10 = 'some_value'\"\n)\n\nThis may seem more complicated at first, and does require some knowledge of SQL, but it is a very efficient way of working with larger datasets, especially in the early stages when you’re still exploring the data and working out what you’re going to do with it."
  },
  {
    "objectID": "posts/the-basics-of-duckdb-in-r/index.html#duckplyr",
    "href": "posts/the-basics-of-duckdb-in-r/index.html#duckplyr",
    "title": "The basics of DuckDB in R",
    "section": "{duckplyr}",
    "text": "{duckplyr}\nA game-changer for me, which really accelerated my adoption of DuckDB as a backend for processing data, was the {duckplyr} package. Those familiar with {dbplyr} will understand the theory behind this package; it allows queries to be built using the standard set of {dplyr} functions, which are converted to SQL behind the scenes.\n\nlibrary(tidyverse)\nlibrary(duckplyr)\n\n# Read CSV using DuckDB behind the scenes\ndata &lt;- read_csv_duckdb(\"some-data-file.csv\")\n\n# Perform data manipulation using dplyr syntax\ndata &lt;- data |&gt;\n    select(col_1, col_2, col_4, col_10) |&gt;\n    filter(col_10 == \"some_value\")\n\nAside from the read_csv_duckdb() function, the rest of the code will be familiar to anyone who has used {dplyr} before. The main advantage of using {duckplyr} over writing SQL and using the {DBI} package is readability - using common {dplyr} functions makes it accessible to a wider range of users. This is a big benefit for teams where not everyone is comfortable reading or writing SQL.\nAdditionally, should the original author fall off the face of the earth, the code is still maintainable by others and readily adapted to eliminate the dependency on DuckDB."
  },
  {
    "objectID": "posts/the-basics-of-duckdb-in-r/index.html#final-thoughts",
    "href": "posts/the-basics-of-duckdb-in-r/index.html#final-thoughts",
    "title": "The basics of DuckDB in R",
    "section": "Final thoughts",
    "text": "Final thoughts\nDuckDB and R are a great combination, allowing me to overcome some of my (self-inflicted?) frustrations in my day-to-day data work. With {duckplyr}, querying data directly from files has smoothed out some of the rough edges in my workflow."
  }
]